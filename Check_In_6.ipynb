{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Check-in 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNfPgpj0OB4I",
    "outputId": "4eb73be2-c9f4-4933-9245-93c16720ba79"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install scikit-lego\n",
    "%pip install seaborn\n",
    "%pip install nbstripout\n",
    "!nbstripout --install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkCVC2020FB4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "df = pd.read_csv(\"./dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVocdikoAasR",
    "outputId": "937ac093-c53f-4f24-9e89-4069f6990e21"
   },
   "outputs": [],
   "source": [
    "# Step 1: Clean Data\n",
    "# Remove duplicates\n",
    "df_cleaned = df.drop(columns='Unnamed: 0').drop_duplicates(subset=['track_id','album_name','artists','track_name'])\n",
    "\n",
    "# Remove columns with every row unique. Also dropping artist and album because it would be too much one-hot encoding\n",
    "df_cleaned.drop(columns=['track_id', 'track_name', 'artists','album_name'], inplace=True)\n",
    "df_cleaned.dropna(axis=0,inplace=True)\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#The columns with object datatype will be categorical\n",
    "columns = df_cleaned.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "df_cleaned = df_cleaned[columns]\n",
    "\n",
    "y = df_cleaned[\"popularity\"]\n",
    "y = np.array([float(i) for i in y])\n",
    "print(y)\n",
    "scaler = StandardScaler() # Scale the data so that the variances for each feature can be similarly weighted\n",
    "df_cleaned = scaler.fit_transform(df_cleaned)\n",
    "df_cleaned = pd.DataFrame(df_cleaned, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We evaluated our NN based on the mean squared error between predicted and true values for popularity on our validation dataset. We also graphed predicted values vs true values for popularity.\n",
    "\n",
    "3. We trained our NN using the built-in learning framework for keras, which utilizes batch gradient descent. Learning rate of 0.001 is recommended for Adam optimizer. Adam will adaptively adjust the learning rate based on an exponentially weighted history of the gradients, so we have \"momentum\" built-in to our learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cleaned.drop(columns=\"popularity\")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(13,)),\n",
    "    keras.layers.Dense(64, activation='relu'),  # Fully connected layer with 64 units\n",
    "    keras.layers.Dense(64, activation='relu'),  # Fully connected layer with 64 units\n",
    "    keras.layers.Dense(1)  # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['root_mean_squared_error'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=25, validation_data=(X_test, y_test))\n",
    "# Plot the training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(X_test).flatten()\n",
    "\n",
    "# Create a DataFrame to compare predictions with real values\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Real Value': y_test,\n",
    "    'Predicted Value': predictions\n",
    "})\n",
    "plt.xlabel(\"True Value\")\n",
    "plt.ylabel(\"Predicted Value\")\n",
    "plt.scatter(y_test, predictions)\n",
    "# Display a sample of the table\n",
    "print(comparison_df.sample(10))  # Show 10 random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
